from GLM_MAB import GLM_MAB, Adversary
from learner import MAB_GridSearch as gs
from math import log, sqrt
import helper
import matplotlib.pyplot as plt
from scipy.stats import bernoulli
import numpy as np, pylab, random
from time import time

my_features = 25

# Generate w_star for adversary
mean = np.zeros(my_features)
covariance = np.random.rand(my_features, my_features)
# covariance = np.dot(covariance, covariance.transpose()) / np.linalg.norm(np.dot(covariance, covariance.transpose()))
covariance = np.dot(covariance, covariance.transpose()) + (float(random.randint(1, 10)) / 10) * np.eye(my_features)
# covariance = covariance / np.linalg.det(covariance)
# w_star = helper.gaussian(mean, covariance, 1)[0]
w_star = np.random.multivariate_normal(mean, covariance, 1)[0]
w_star = w_star / np.linalg.norm(w_star)
# adversary = Adversary(w_star, 0.0)
adversary = Adversary(w_star, 0.0, model = "linear regression")
# Get arms
# X = helper.gaussian(mean, covariance, 100)
X = np.random.multivariate_normal(mean, covariance, 100)
n_samples, n_features = X.shape
for i in range(n_samples):
	X[i] = X[i] / np.linalg.norm(X[i])

a_star = X[0]
a_star_reward = adversary.get_adversary_reward(X[0])
ai = 0
for i in range(1, n_samples):
	cur_reward = adversary.get_adversary_reward(X[i])
	# print cur_reward
	if(cur_reward > a_star_reward):
		a_star_reward = cur_reward
		a_star = X[i]
		ai = i
adversary.a_star_reward = a_star_reward

# model = GLM_MAB(arms = X)
#
# # Initialize grid search
# params = {'ro' : [float(i + 1) / 100.0 for i in range(10, 101, 10)]}
# gs_model = gs(model, adversary, params, 100, plot = plt)
#
# # Sample some points for warm start
# dataset = {}
# dataset["ip"] = []
# dataset["op"] = []
# Y = []
# t = 50
# h = np.zeros(n_samples)
# regret = 0.0
# y_plot = []
# x_plot = []
# for i in range(t):
# 	j = random.randint(0, 99)
# 	while(h[j] == 1):
# 		j = random.randint(0, 99)
# 	h[j] = 1
# 	dataset["ip"].append(model.arms[j])
# 	y = adversary.get_adversary_reward(model.arms[j])
# 	regret += a_star_reward - y
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	Y.append(y)
# 	dataset["op"].append(bernoulli.rvs(y))
# dataset["ip"] = np.array(dataset["ip"])
# dataset["op"] = np.array(dataset["op"])
# model.update_matrix(dataset["ip"])
#
# # Fit the model to get best estimator
# model = gs_model.fit(dataset["ip"], dataset["op"]).best_estimator_
# print "ro parameter for UCB = ", model.ro
# const = gs_model.best_const
# print "Best const config = ", const
#
# # Train on this best model
# # model.fit(dataset["ip"], dataset["op"])
#
# # regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
# print regret
# # x_plot.append(50)
# # y_plot.append(regret)
#
# for i in range(t, 1000):
# 	next_arm = model.predict_arm(model.acquisition)					# Predit arm
# 	model.update_matrix(next_arm)					# Update design matrix
# 	# chosen.append(np.where(model.arms == next_arm)[0][0])
# 	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary
#
# 	Y1 = list(dataset["op"])
# 	X1 = list(dataset["ip"])
# 	X1.append(next_arm)
# 	Y.append(y)
# 	Y1.append(bernoulli.rvs(y))
# 	dataset["ip"] = np.array(X1)
# 	dataset["op"] = np.array(Y1)
#
# 	model.fit(dataset["ip"], dataset["op"])
# 	regret += a_star_reward - y
# 	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	params = {'ro' : sqrt(const * log(np.linalg.norm(model.M_)))}
# 	model.set_params(params)
#
# plt.subplot(2, 1, 2)
# plt.plot(x_plot, y_plot, 'red')
# plt.ylabel('Regret')
# plt.xlabel('Time step')
# plt.title('Chosen const = ' + str(const))


# model = GLM_MAB(arms = X, algo = 'lazy_TS')
# model = GLM_MAB(arms = X, algo = 'lazy_TS', solver = "linear regression")
model = GLM_MAB(arms = X, algo = 'lazy_TS', nu = 0.01, solver = "linear regression")

# # Initialize grid search
# params = {'const' : [float(i) / 100.0 for i in range(1, 11)]}
# # params = {'nu' : [float(i) / 10.0 for i in range(1, 11)]}
# gs_model = gs(model, adversary, params, 1000, plot = plt)

# Sample some points for warm start
dataset = {}
dataset["ip"] = []
dataset["op"] = []
Y = []
t = 50
h = np.zeros(n_samples)
regret = 0.0
y_plot = []
x_plot = []
for i in range(t):
	j = random.randint(0, 99)
	while(h[j] == 1):
		j = random.randint(0, 99)
	h[j] = 1
	dataset["ip"].append(model.arms[j])
	y = adversary.get_adversary_reward(model.arms[j])
	regret += a_star_reward - y
	Y.append(y)
	x_plot.append(i + 1)
	y_plot.append(regret)
	# dataset["op"].append(bernoulli.rvs(y))
	dataset["op"].append(y)
dataset["ip"] = np.array(dataset["ip"])
dataset["op"] = np.array(dataset["op"])
model.update_matrix(dataset["ip"])

# # Fit the model to get best estimator
# model = gs_model.fit(dataset["ip"], dataset["op"]).best_estimator_
# print "nu parameter for TS = ", model.nu
# const = gs_model.best_const
# delta = gs_model.best_delta
# print "Best const config = ", const

# Train on this best model
model.fit(dataset["ip"], dataset["op"])

# regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
print regret
# x_plot.append(50)
# y_plot.append(regret)

for i in range(t, 10000):
	next_arm = model.predict_arm(model.acquisition)					# Predit arm
	model.update_matrix(next_arm)					# Update design matrix
	# chosen.append(np.where(model.arms == next_arm)[0][0])
	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

	Y1 = list(dataset["op"])
	X1 = list(dataset["ip"])
	X1.append(next_arm)
	Y.append(y)
	# Y1.append(bernoulli.rvs(y))
	Y1.append(y)
	dataset["ip"] = np.array(X1)
	dataset["op"] = np.array(Y1)

	model.fit(dataset["ip"], dataset["op"])
	# regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
	regret += a_star_reward - y
	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
	# params = {'nu' : const *  9 * n_features * log(i / delta)}
	# params = {'nu' : const * log(i)}
	# model.set_params(params)
	x_plot.append(i + 1)
	y_plot.append(regret)

# plt.subplot(2, 1, 2)
plt.plot(x_plot, y_plot, 'blue')
plt.ylabel('Cumulative Regret')
plt.xlabel('Time step')
# plt.title('Chosen const = ' + str(const) + "and chosen delta = " + str(delta))
# plt.title('Chosen const = ' + str(const))




# model = GLM_MAB(arms = X, algo = 'UCB')
# w_hat = helper.gaussian(mean, covariance, 1)[0]
# model.w_hat = w_hat / np.linalg.norm(w_hat)
# regret = 0.0
# y_plot = []
# x_plot = []
#
# # Initialize grid search
# # params = {'ro' : [float(i + 1) / 100.0 for i in range(10, 101, 10)]}
# # gs_model = gs(model, adversary, params, 100, plot = plt)
# #
# # # Sample some points for warm start
# # dataset = {}
# # dataset["ip"] = []
# # dataset["op"] = []
# #
# # # Fit the model to get best estimator
# # model = gs_model.fit(dataset["ip"], dataset["op"]).best_estimator_
# # print "ro parameter for UCB = ", model.ro
# # const = gs_model.best_const
# # print "Best const config = ", const
# t = 50
# n_samples, n_features = model.arms.shape
# h = np.zeros(n_samples)
# for i in range(t):
# 	j = random.randint(0, n_samples - 1)
# 	while(h[j] == 1):
# 		j = random.randint(0, n_samples - 1)
# 	h[j] = 1
# 	y = adversary.get_adversary_reward(model.arms[j])
# 	regret += a_star_reward - y
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	# print model.arms[j].reshape(1, 1000).shape
# 	model.update_matrix(model.arms[j].reshape(1, 1000))
# 	vec = np.dot(model.M_, model.arms[j].reshape(n_features,))
# 	model.w_hat += vec / np.linalg.norm(vec)
# 	model.w_hat /= np.linalg.norm(model.w_hat)
#
# print regret
#
# for i in range(t, 1000):
# 	next_arm = model.predict_arm(model.acquisition)					# Predit arm
# 	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary
#
# 	regret += a_star_reward - y
# 	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	# params = {'ro' : sqrt(const * log(np.linalg.norm(model.M_)))}
# 	# model.set_params(params)
#
# # plt.subplot(2, 1, 2)
# plt.plot(x_plot, y_plot, 'red')
# plt.ylabel('Regret')
# plt.xlabel('Time step')
# plt.title('Chosen const = ' + str(const))
plt.show()


# Lists for plot
y_plots = []
x_plots = []
y_plot = []
x_plot = []
# bi = 0


colors = ['red', 'green', 'blue', 'brown']
labels = []
print "iter ", 1
const = .7
labels.append("const = " + str(const))
# print("Creating object")
# ro = sqrt(const * log(50))
ro = sqrt(const * log(np.linalg.norm(Des_mat)))

model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0], ro = ro)
# print("object created")
# print(hasattr(model, 'get_params'))


dataset = {}
dataset["ip"] = []
dataset["op"] = []
Y = []
t = 50
h = np.zeros(1000)
for i in range(t):
	j = random.randint(0, 99)
	while(h[j] == 1):
		j = random.randint(0, 99)
	h[j] = 1
	dataset["ip"].append(model.arms[j])
	y = adversary.get_adversary_reward(model.arms[j])
	Y.append(y)
	dataset["op"].append(bernoulli.rvs(y))
dataset["ip"] = np.array(dataset["ip"])
dataset["op"] = np.array(dataset["op"])
model.update_matrix(dataset["ip"])
# print("matrix updated")
model.fit(dataset["ip"], dataset["op"])
back_up = dataset
buy = Y
chosen = []
regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
print regret
x_plot.append(50)
y_plot.append(regret)

for i in range(t, 1000):
	# if(i % 100 == 0):
	# 	print "Doing %d"%i

	# ro = sqrt(const * log(i + 1))
	# param = {'ro' : ro}
	# model = model.set_params(param)
	# print model.solver

	# Predicting and sampling the next best arm
	next_arm = model.predict_arm(model.acquisition)					# Predit arm
	model.update_matrix(next_arm)					# Update design matrix
	chosen.append(np.where(model.arms == next_arm)[0][0])
	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

	Y1 = list(dataset["op"])
	X1 = list(dataset["ip"])
	X1.append(next_arm)
	Y.append(y)
	Y1.append(bernoulli.rvs(y))
	dataset["ip"] = np.array(X1)
	dataset["op"] = np.array(Y1)

	model.fit(dataset["ip"], dataset["op"])
	regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
	x_plot.append(i + 1)
	y_plot.append(regret)

	# Update ro for next iteration
	Des_mat = Des_mat + np.outer(next_arm, next_arm.transpose())
	ro = sqrt(const * log(np.linalg.norm(Des_mat)))
	param = {'ro' : ro}
	model = model.set_params(param)
x_plots.append(x_plot)
# avg_reg = .001 * cum_regret
# print avg_reg
y_plots.append(y_plot)





for const in [1, 2, 5]:
	labels.append("const = " + str(const))
	y_plot = []
	x_plot = []
	dataset = back_up
	Des_mat = back_des_mat
	Y = buy
	print "iter ", const
	# ro = sqrt(const * log(50))
	ro = sqrt(const * log(np.linalg.norm(Des_mat)))
	model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0], ro = ro)
	t = 50

	model.update_matrix(dataset["ip"])
	model.fit(dataset["ip"], dataset["op"])
	regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
	print regret
	x_plot.append(50)
	y_plot.append(regret)

	for i in range(t, 1000):
		# ro = sqrt(const * log(i + 1))
		# param = {'ro' : ro}
		# model = model.set_params(param)

		# Predicting and sampling the next best arm
		next_arm = model.predict_arm(model.acquisition)					# Predit arm
		model.update_matrix(next_arm)					# Update design matrix
		chosen.append(np.where(model.arms == next_arm)[0][0])
		y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

		Y1 = list(dataset["op"])
		X1 = list(dataset["ip"])
		X1.append(next_arm)
		Y.append(y)
		Y1.append(bernoulli.rvs(y))
		dataset["ip"] = np.array(X1)
		dataset["op"] = np.array(Y1)

		model.fit(dataset["ip"], dataset["op"])
		regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
		print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
		x_plot.append(i + 1)
		y_plot.append(regret)

		# Update ro for next iteration
		Des_mat = Des_mat + np.outer(next_arm, next_arm.transpose())
		ro = sqrt(const * log(np.linalg.norm(Des_mat)))
		param = {'ro' : ro}
		model = model.set_params(param)
	x_plots.append(x_plot)
	y_plots.append(y_plot)

for i in range(len(x_plots)):
    plt.plot(x_plots[i], y_plots[i], colors[i], label = labels[i])


# plt.title('Average Regret curve at different values for const')
# pylab.xlim([0.1, 1])
plt.ylabel('Regret')
plt.xlabel('Time step')
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode="expand", borderaxespad=0.)
plt.show()




# X = np.random.multivariate_normal(mean, covariance, size = 1000)
# Y = np.array([get_adversary_reward("logistic", x, w_star) for x in X])

# Sample some(say, 50) data points in the beginning
# for i in range(t):
# 	y = get_adversary_reward("logistic", X[0], w_star)
# 	dataset["ip"].append(x)
# 	dataset["op"].append(y)






# Estimate an initial w_hat using the above sampled data points
# estimator = Estimator(dataset)
# w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0]
# w_hat = estimator.estimate("Gradient Descent", w_hat, D)			# Estimate w_hat using gradient descent
# epsilon = np.linalg.norm(w_star - w_hat)
# print "Distance = ", epsilon
# y_plot.append(epsilon)
# x_plot.append(i - 1)

# print "lengths = ", len(estimator.dataset["ip"]), len(dataset["ip"])

# for i in range(50, 1000):
# 	print "Doing ", i
# 	x = np.random.multivariate_normal(mean, covariance, size = 1)[0]
# 	y = get_adversary_reward("logistic", x, w_star)
# 	X1 = list(X)
# 	Y1 = list(Y)
# 	X1.append(x)
# 	Y1.append(y)
# 	X = np.array(X1)
# 	Y = np.array(Y1)
# 	model.fit(X,Y)
# 	w_hat = model.coef_
# 	epsilon = np.linalg.norm(w_star - w_hat)
# 	print "Distance = ", epsilon
# 	y_plot.append(epsilon)
# 	x_plot.append(i)
# 	estimator.dataset["ip"].append(x)
# 	estimator.dataset["op"].append(y)
# 	# print "New length = ", len(estimator.dataset["ip"])
# 	w_hat = estimator.estimate("Gradient Descent", w_hat, D)
# 	epsilon = np.linalg.norm(w_star - w_hat)
# 	# print "Distance = ", epsilon
# 	y_plot.append(epsilon)
# 	x_plot.append(i)





# def acquisition(i, ro):
# 	mu = helper.get_link_val("logistic", X[i], w_hat)
# 	explore = ro * np.dot(np.dot(X[i], M_inv), X[i].transpose())
#
# 	return mu + explore

# def get_best_arm(ro):
# 	max = acquisition(0, ro)
# 	mi = 0
# 	for i in range(1, 1000):
# 		score = acquisition(i, ro)
# 		if(max < score):
# 			max = score
# 			mi = i
#
# 	return mi




# def get_link_val(link, *argv):
# 	p = helper.link_func(link, *argv)
# 	return p
# print p
# return bernoulli.rvs(p)	# + np.random.normal(0, 1, 1)[0]






# def get_adversary_reward(link, *argv):
# 	argv.append(w_star)
# 	return helper.link_func(link, *argv)


# add = np.outer(X[j], X[j].transpose())
# M = M + add
# M_inv = update_mat_inv(M_inv, add)


# epsilon = np.linalg.norm(w_star - w_hat)
# print "Distance = ", epsilon
# y_plot.append(epsilon)
# x_plot.append(49)






# TO BE INCORPORATED LATER
'''
while (eplsilon >= threshold and t < 100000):
	global epsilon, t
	x = choose_arm()
	y = get_adversary_reward("logistic", x, w_star)
	estimator.dataset["ip"].append(x)
	estimator.dataset["op"].append(y)
	w_hat = estimator.estimate("Gradient Descent")
	epsilon = np.linalg.norm(w_star - w_hat)
	t += 1
	yplot.append(epsilon)
	xplot.append(t)

'''







print("Creating object")
model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0])
print("object created")
print(hasattr(model, 'get_params'))

# Lists for plot
y_plot = []
x_plot = []

dataset = {}
dataset["ip"] = []
dataset["op"] = []
Y = []
t = 50

for i in range(t):
	j = random.randint(0, 999)
	# M = M + np.outer(X[j], X[j].transpose())
	dataset["ip"].append(model.arms[j])
	y = adversary.get_adversary_reward(model.arms[j])
	Y.append(y)
	dataset["op"].append(bernoulli.rvs(y))

dataset["ip"] = np.array(dataset["ip"])
dataset["op"] = np.array(dataset["op"])
model.update_matrix(dataset["ip"])
print("matrix updated")
t_term = log(t)
ro = [sqrt((float(ro_i) / 100) * t_term) for ro_i in range(1, 10)]
print ro[:10]
# random.shuffle(ro)
# print ro[:10]
parameters = {'ro' : ro}
print("parameters set for grid search")
# TO DO FIT THE MODEL USING GRID SEARCH
grid = GridSearchCV(model, param_grid=parameters)
print("grid search object created")
grid.fit(dataset["ip"], dataset["op"])
print("fitting dataset")
print "best ro = ", grid.best_estimator_.ro
chosen = []

for i in range(t, 10001):
	print "Doing %d"%i

	# CALCULATION TO BE DONE
	t_term = log(i + 1)
	# print t_term
	ro = [sqrt((float(ro_i) / 100) * t_term) for ro_i in range(1, 10)]
	print ro[:10]
	# random.shuffle(ro)
	# print ro[:10]
	parameters = {'ro' : ro}

	# Predicting and sampling the next best arm
	next_arm = model.predict_arm(model.acquisition)					# Predit arm
	model.update_matrix(next_arm)					# Update design matrix
	chosen.append(np.where(model.arms == next_arm)[0][0])
	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary
	# Update the dataset with this sample
	Y1 = list(dataset["op"])
	X1 = list(dataset["ip"])
	X1.append(X[j])
	Y.append(y)
	Y1.append(bernoulli.rvs(y))
	dataset["ip"] = np.array(X1)
	dataset["op"] = np.array(Y1)


	# TO DO FIT THE MODEL USING GRID SEARCH
	grid = GridSearchCV(model, param_grid=parameters)
	grid.fit(dataset["ip"], dataset["op"])
	print "best ro = ", grid.best_estimator_.ro

	x_plot.append(i)
w_sort = [i[0] for i in sorted(enumerate(list(adversary.w_star_)), key=lambda x:x[1])]
print w[:10]
print "best ro = ", grid.best_estimator_.ro
plt.plot(x_plot, chosen, 'ro')
plt.show()









for ro_k in range(1, 11):
	print "iter ", ro_k
	const = float(ro_k) / 10
	# print("Creating object")
	ro = sqrt(const * log(50))
	model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0], ro = ro)
	# print("object created")
	# print(hasattr(model, 'get_params'))


	dataset = {}
	dataset["ip"] = []
	dataset["op"] = []
	Y = []
	t = 50

	for i in range(t):
		j = random.randint(0, 999)
		dataset["ip"].append(model.arms[j])
		y = adversary.get_adversary_reward(model.arms[j])
		Y.append(y)
		dataset["op"].append(bernoulli.rvs(y))

	dataset["ip"] = np.array(dataset["ip"])
	dataset["op"] = np.array(dataset["op"])
	model.update_matrix(dataset["ip"])
	# print("matrix updated")
	model.fit(dataset["ip"], dataset["op"])
	chosen = []
	cum_regret = np.linalg.norm(adversary.w_star_ - model.w_hat)

	for i in range(t, 1000):
		# if(i % 100 == 0):
		# 	print "Doing %d"%i

		ro = sqrt(const * log(i + 1))
		param = {'ro' : ro}
		model = model.set_params(param)
		# print model.solver

		# Predicting and sampling the next best arm
		next_arm = model.predict_arm(model.acquisition)					# Predit arm
		model.update_matrix(next_arm)					# Update design matrix
		chosen.append(np.where(model.arms == next_arm)[0][0])
		y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

		Y1 = list(dataset["op"])
		X1 = list(dataset["ip"])
		X1.append(X[j])
		Y.append(y)
		Y1.append(bernoulli.rvs(y))
		dataset["ip"] = np.array(X1)
		dataset["op"] = np.array(Y1)

		model.fit(dataset["ip"], dataset["op"])
		cum_regret += np.linalg.norm(adversary.w_star_ - model.w_hat)
	x_plot.append(const)
	avg_reg = .001 * cum_regret
	print avg_reg
	y_plot.append(avg_reg)





































HELPER BACK UP






'''
   FUNCTION TO GENERATE DATASET FOR THE ADVERSARY

   ARGUMENTS: NONE

   RETURNS: DICT
	Dataset, which is a dictionary with input point as key and output as value

def simulate_data():
	# Fix a w_star
	mean = np.zeros(1000)
	covariance = np.random.rand(1000, 1000)
	w_star = np.random.multivariate_normal(mean, covariance, size = 1)[0]

	dataset = {}
	for i in range(10000):
		while (True):
			x = np.random.multivariate_normal(mean, covariance, size = 1)[0]		# Pick an input
			if (x not in dataset):
				y = link_func("logistic", x, w_star) + np.random.normal(0, 1, 1)[0]             # Generate it's output
				dataset[x] = y
				break

	return dataset, w_star
'''
