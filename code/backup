from GLM_MAB import GLM_MAB, Adversary
from learner import MAB_GridSearch as gs
from math import log, sqrt
import helper
import matplotlib.pyplot as plt
from scipy.stats import bernoulli
import numpy as np, pylab, random
from time import time

my_features = 25

# Generate w_star for adversary
mean = np.zeros(my_features)
covariance = np.random.rand(my_features, my_features)
# covariance = np.dot(covariance, covariance.transpose()) / np.linalg.norm(np.dot(covariance, covariance.transpose()))
covariance = np.dot(covariance, covariance.transpose()) + (float(random.randint(1, 10)) / 10) * np.eye(my_features)
# covariance = covariance / np.linalg.det(covariance)
# w_star = helper.gaussian(mean, covariance, 1)[0]
w_star = np.random.multivariate_normal(mean, covariance, 1)[0]
w_star = w_star / np.linalg.norm(w_star)
# adversary = Adversary(w_star, 0.0)
adversary = Adversary(w_star, 0.0, model = "linear regression")
# Get arms
# X = helper.gaussian(mean, covariance, 100)
X = np.random.multivariate_normal(mean, covariance, 100)
n_samples, n_features = X.shape
for i in range(n_samples):
	X[i] = X[i] / np.linalg.norm(X[i])

a_star = X[0]
a_star_reward = adversary.get_adversary_reward(X[0])
ai = 0
for i in range(1, n_samples):
	cur_reward = adversary.get_adversary_reward(X[i])
	# print cur_reward
	if(cur_reward > a_star_reward):
		a_star_reward = cur_reward
		a_star = X[i]
		ai = i
adversary.a_star_reward = a_star_reward

# model = GLM_MAB(arms = X)
#
# # Initialize grid search
# params = {'ro' : [float(i + 1) / 100.0 for i in range(10, 101, 10)]}
# gs_model = gs(model, adversary, params, 100, plot = plt)
#
# # Sample some points for warm start
# dataset = {}
# dataset["ip"] = []
# dataset["op"] = []
# Y = []
# t = 50
# h = np.zeros(n_samples)
# regret = 0.0
# y_plot = []
# x_plot = []
# for i in range(t):
# 	j = random.randint(0, 99)
# 	while(h[j] == 1):
# 		j = random.randint(0, 99)
# 	h[j] = 1
# 	dataset["ip"].append(model.arms[j])
# 	y = adversary.get_adversary_reward(model.arms[j])
# 	regret += a_star_reward - y
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	Y.append(y)
# 	dataset["op"].append(bernoulli.rvs(y))
# dataset["ip"] = np.array(dataset["ip"])
# dataset["op"] = np.array(dataset["op"])
# model.update_matrix(dataset["ip"])
#
# # Fit the model to get best estimator
# model = gs_model.fit(dataset["ip"], dataset["op"]).best_estimator_
# print "ro parameter for UCB = ", model.ro
# const = gs_model.best_const
# print "Best const config = ", const
#
# # Train on this best model
# # model.fit(dataset["ip"], dataset["op"])
#
# # regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
# print regret
# # x_plot.append(50)
# # y_plot.append(regret)
#
# for i in range(t, 1000):
# 	next_arm = model.predict_arm(model.acquisition)					# Predit arm
# 	model.update_matrix(next_arm)					# Update design matrix
# 	# chosen.append(np.where(model.arms == next_arm)[0][0])
# 	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary
#
# 	Y1 = list(dataset["op"])
# 	X1 = list(dataset["ip"])
# 	X1.append(next_arm)
# 	Y.append(y)
# 	Y1.append(bernoulli.rvs(y))
# 	dataset["ip"] = np.array(X1)
# 	dataset["op"] = np.array(Y1)
#
# 	model.fit(dataset["ip"], dataset["op"])
# 	regret += a_star_reward - y
# 	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	params = {'ro' : sqrt(const * log(np.linalg.norm(model.M_)))}
# 	model.set_params(params)
#
# plt.subplot(2, 1, 2)
# plt.plot(x_plot, y_plot, 'red')
# plt.ylabel('Regret')
# plt.xlabel('Time step')
# plt.title('Chosen const = ' + str(const))


# model = GLM_MAB(arms = X, algo = 'lazy_TS')
# model = GLM_MAB(arms = X, algo = 'lazy_TS', solver = "linear regression")
model = GLM_MAB(arms = X, algo = 'lazy_TS', nu = 0.01, solver = "linear regression")

# # Initialize grid search
# params = {'const' : [float(i) / 100.0 for i in range(1, 11)]}
# # params = {'nu' : [float(i) / 10.0 for i in range(1, 11)]}
# gs_model = gs(model, adversary, params, 1000, plot = plt)

# Sample some points for warm start
dataset = {}
dataset["ip"] = []
dataset["op"] = []
Y = []
t = 50
h = np.zeros(n_samples)
regret = 0.0
y_plot = []
x_plot = []
for i in range(t):
	j = random.randint(0, 99)
	while(h[j] == 1):
		j = random.randint(0, 99)
	h[j] = 1
	dataset["ip"].append(model.arms[j])
	y = adversary.get_adversary_reward(model.arms[j])
	regret += a_star_reward - y
	Y.append(y)
	x_plot.append(i + 1)
	y_plot.append(regret)
	# dataset["op"].append(bernoulli.rvs(y))
	dataset["op"].append(y)
dataset["ip"] = np.array(dataset["ip"])
dataset["op"] = np.array(dataset["op"])
model.update_matrix(dataset["ip"])

# # Fit the model to get best estimator
# model = gs_model.fit(dataset["ip"], dataset["op"]).best_estimator_
# print "nu parameter for TS = ", model.nu
# const = gs_model.best_const
# delta = gs_model.best_delta
# print "Best const config = ", const

# Train on this best model
model.fit(dataset["ip"], dataset["op"])

# regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
print regret
# x_plot.append(50)
# y_plot.append(regret)

for i in range(t, 10000):
	next_arm = model.predict_arm(model.acquisition)					# Predit arm
	model.update_matrix(next_arm)					# Update design matrix
	# chosen.append(np.where(model.arms == next_arm)[0][0])
	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

	Y1 = list(dataset["op"])
	X1 = list(dataset["ip"])
	X1.append(next_arm)
	Y.append(y)
	# Y1.append(bernoulli.rvs(y))
	Y1.append(y)
	dataset["ip"] = np.array(X1)
	dataset["op"] = np.array(Y1)

	model.fit(dataset["ip"], dataset["op"])
	# regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
	regret += a_star_reward - y
	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
	# params = {'nu' : const *  9 * n_features * log(i / delta)}
	# params = {'nu' : const * log(i)}
	# model.set_params(params)
	x_plot.append(i + 1)
	y_plot.append(regret)

# plt.subplot(2, 1, 2)
plt.plot(x_plot, y_plot, 'blue')
plt.ylabel('Cumulative Regret')
plt.xlabel('Time step')
# plt.title('Chosen const = ' + str(const) + "and chosen delta = " + str(delta))
# plt.title('Chosen const = ' + str(const))




# model = GLM_MAB(arms = X, algo = 'UCB')
# w_hat = helper.gaussian(mean, covariance, 1)[0]
# model.w_hat = w_hat / np.linalg.norm(w_hat)
# regret = 0.0
# y_plot = []
# x_plot = []
#
# # Initialize grid search
# # params = {'ro' : [float(i + 1) / 100.0 for i in range(10, 101, 10)]}
# # gs_model = gs(model, adversary, params, 100, plot = plt)
# #
# # # Sample some points for warm start
# # dataset = {}
# # dataset["ip"] = []
# # dataset["op"] = []
# #
# # # Fit the model to get best estimator
# # model = gs_model.fit(dataset["ip"], dataset["op"]).best_estimator_
# # print "ro parameter for UCB = ", model.ro
# # const = gs_model.best_const
# # print "Best const config = ", const
# t = 50
# n_samples, n_features = model.arms.shape
# h = np.zeros(n_samples)
# for i in range(t):
# 	j = random.randint(0, n_samples - 1)
# 	while(h[j] == 1):
# 		j = random.randint(0, n_samples - 1)
# 	h[j] = 1
# 	y = adversary.get_adversary_reward(model.arms[j])
# 	regret += a_star_reward - y
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	# print model.arms[j].reshape(1, 1000).shape
# 	model.update_matrix(model.arms[j].reshape(1, 1000))
# 	vec = np.dot(model.M_, model.arms[j].reshape(n_features,))
# 	model.w_hat += vec / np.linalg.norm(vec)
# 	model.w_hat /= np.linalg.norm(model.w_hat)
#
# print regret
#
# for i in range(t, 1000):
# 	next_arm = model.predict_arm(model.acquisition)					# Predit arm
# 	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary
#
# 	regret += a_star_reward - y
# 	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
# 	x_plot.append(i + 1)
# 	y_plot.append(regret)
# 	# params = {'ro' : sqrt(const * log(np.linalg.norm(model.M_)))}
# 	# model.set_params(params)
#
# # plt.subplot(2, 1, 2)
# plt.plot(x_plot, y_plot, 'red')
# plt.ylabel('Regret')
# plt.xlabel('Time step')
# plt.title('Chosen const = ' + str(const))
plt.show()


# Lists for plot
y_plots = []
x_plots = []
y_plot = []
x_plot = []
# bi = 0


colors = ['red', 'green', 'blue', 'brown']
labels = []
print "iter ", 1
const = .7
labels.append("const = " + str(const))
# print("Creating object")
# ro = sqrt(const * log(50))
ro = sqrt(const * log(np.linalg.norm(Des_mat)))

model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0], ro = ro)
# print("object created")
# print(hasattr(model, 'get_params'))


dataset = {}
dataset["ip"] = []
dataset["op"] = []
Y = []
t = 50
h = np.zeros(1000)
for i in range(t):
	j = random.randint(0, 99)
	while(h[j] == 1):
		j = random.randint(0, 99)
	h[j] = 1
	dataset["ip"].append(model.arms[j])
	y = adversary.get_adversary_reward(model.arms[j])
	Y.append(y)
	dataset["op"].append(bernoulli.rvs(y))
dataset["ip"] = np.array(dataset["ip"])
dataset["op"] = np.array(dataset["op"])
model.update_matrix(dataset["ip"])
# print("matrix updated")
model.fit(dataset["ip"], dataset["op"])
back_up = dataset
buy = Y
chosen = []
regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
print regret
x_plot.append(50)
y_plot.append(regret)

for i in range(t, 1000):
	# if(i % 100 == 0):
	# 	print "Doing %d"%i

	# ro = sqrt(const * log(i + 1))
	# param = {'ro' : ro}
	# model = model.set_params(param)
	# print model.solver

	# Predicting and sampling the next best arm
	next_arm = model.predict_arm(model.acquisition)					# Predit arm
	model.update_matrix(next_arm)					# Update design matrix
	chosen.append(np.where(model.arms == next_arm)[0][0])
	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

	Y1 = list(dataset["op"])
	X1 = list(dataset["ip"])
	X1.append(next_arm)
	Y.append(y)
	Y1.append(bernoulli.rvs(y))
	dataset["ip"] = np.array(X1)
	dataset["op"] = np.array(Y1)

	model.fit(dataset["ip"], dataset["op"])
	regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
	print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
	x_plot.append(i + 1)
	y_plot.append(regret)

	# Update ro for next iteration
	Des_mat = Des_mat + np.outer(next_arm, next_arm.transpose())
	ro = sqrt(const * log(np.linalg.norm(Des_mat)))
	param = {'ro' : ro}
	model = model.set_params(param)
x_plots.append(x_plot)
# avg_reg = .001 * cum_regret
# print avg_reg
y_plots.append(y_plot)





for const in [1, 2, 5]:
	labels.append("const = " + str(const))
	y_plot = []
	x_plot = []
	dataset = back_up
	Des_mat = back_des_mat
	Y = buy
	print "iter ", const
	# ro = sqrt(const * log(50))
	ro = sqrt(const * log(np.linalg.norm(Des_mat)))
	model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0], ro = ro)
	t = 50

	model.update_matrix(dataset["ip"])
	model.fit(dataset["ip"], dataset["op"])
	regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
	print regret
	x_plot.append(50)
	y_plot.append(regret)

	for i in range(t, 1000):
		# ro = sqrt(const * log(i + 1))
		# param = {'ro' : ro}
		# model = model.set_params(param)

		# Predicting and sampling the next best arm
		next_arm = model.predict_arm(model.acquisition)					# Predit arm
		model.update_matrix(next_arm)					# Update design matrix
		chosen.append(np.where(model.arms == next_arm)[0][0])
		y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

		Y1 = list(dataset["op"])
		X1 = list(dataset["ip"])
		X1.append(next_arm)
		Y.append(y)
		Y1.append(bernoulli.rvs(y))
		dataset["ip"] = np.array(X1)
		dataset["op"] = np.array(Y1)

		model.fit(dataset["ip"], dataset["op"])
		regret = np.linalg.norm(adversary.w_star_ - model.w_hat)
		print "regret = " + str(regret) + "\twhile taking " + str(i) + "th step"
		x_plot.append(i + 1)
		y_plot.append(regret)

		# Update ro for next iteration
		Des_mat = Des_mat + np.outer(next_arm, next_arm.transpose())
		ro = sqrt(const * log(np.linalg.norm(Des_mat)))
		param = {'ro' : ro}
		model = model.set_params(param)
	x_plots.append(x_plot)
	y_plots.append(y_plot)

for i in range(len(x_plots)):
    plt.plot(x_plots[i], y_plots[i], colors[i], label = labels[i])


# plt.title('Average Regret curve at different values for const')
# pylab.xlim([0.1, 1])
plt.ylabel('Regret')
plt.xlabel('Time step')
plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode="expand", borderaxespad=0.)
plt.show()




# X = np.random.multivariate_normal(mean, covariance, size = 1000)
# Y = np.array([get_adversary_reward("logistic", x, w_star) for x in X])

# Sample some(say, 50) data points in the beginning
# for i in range(t):
# 	y = get_adversary_reward("logistic", X[0], w_star)
# 	dataset["ip"].append(x)
# 	dataset["op"].append(y)






# Estimate an initial w_hat using the above sampled data points
# estimator = Estimator(dataset)
# w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0]
# w_hat = estimator.estimate("Gradient Descent", w_hat, D)			# Estimate w_hat using gradient descent
# epsilon = np.linalg.norm(w_star - w_hat)
# print "Distance = ", epsilon
# y_plot.append(epsilon)
# x_plot.append(i - 1)

# print "lengths = ", len(estimator.dataset["ip"]), len(dataset["ip"])

# for i in range(50, 1000):
# 	print "Doing ", i
# 	x = np.random.multivariate_normal(mean, covariance, size = 1)[0]
# 	y = get_adversary_reward("logistic", x, w_star)
# 	X1 = list(X)
# 	Y1 = list(Y)
# 	X1.append(x)
# 	Y1.append(y)
# 	X = np.array(X1)
# 	Y = np.array(Y1)
# 	model.fit(X,Y)
# 	w_hat = model.coef_
# 	epsilon = np.linalg.norm(w_star - w_hat)
# 	print "Distance = ", epsilon
# 	y_plot.append(epsilon)
# 	x_plot.append(i)
# 	estimator.dataset["ip"].append(x)
# 	estimator.dataset["op"].append(y)
# 	# print "New length = ", len(estimator.dataset["ip"])
# 	w_hat = estimator.estimate("Gradient Descent", w_hat, D)
# 	epsilon = np.linalg.norm(w_star - w_hat)
# 	# print "Distance = ", epsilon
# 	y_plot.append(epsilon)
# 	x_plot.append(i)





# def acquisition(i, ro):
# 	mu = helper.get_link_val("logistic", X[i], w_hat)
# 	explore = ro * np.dot(np.dot(X[i], M_inv), X[i].transpose())
#
# 	return mu + explore

# def get_best_arm(ro):
# 	max = acquisition(0, ro)
# 	mi = 0
# 	for i in range(1, 1000):
# 		score = acquisition(i, ro)
# 		if(max < score):
# 			max = score
# 			mi = i
#
# 	return mi




# def get_link_val(link, *argv):
# 	p = helper.link_func(link, *argv)
# 	return p
# print p
# return bernoulli.rvs(p)	# + np.random.normal(0, 1, 1)[0]






# def get_adversary_reward(link, *argv):
# 	argv.append(w_star)
# 	return helper.link_func(link, *argv)


# add = np.outer(X[j], X[j].transpose())
# M = M + add
# M_inv = update_mat_inv(M_inv, add)


# epsilon = np.linalg.norm(w_star - w_hat)
# print "Distance = ", epsilon
# y_plot.append(epsilon)
# x_plot.append(49)






# TO BE INCORPORATED LATER
'''
while (eplsilon >= threshold and t < 100000):
	global epsilon, t
	x = choose_arm()
	y = get_adversary_reward("logistic", x, w_star)
	estimator.dataset["ip"].append(x)
	estimator.dataset["op"].append(y)
	w_hat = estimator.estimate("Gradient Descent")
	epsilon = np.linalg.norm(w_star - w_hat)
	t += 1
	yplot.append(epsilon)
	xplot.append(t)

'''







print("Creating object")
model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0])
print("object created")
print(hasattr(model, 'get_params'))

# Lists for plot
y_plot = []
x_plot = []

dataset = {}
dataset["ip"] = []
dataset["op"] = []
Y = []
t = 50

for i in range(t):
	j = random.randint(0, 999)
	# M = M + np.outer(X[j], X[j].transpose())
	dataset["ip"].append(model.arms[j])
	y = adversary.get_adversary_reward(model.arms[j])
	Y.append(y)
	dataset["op"].append(bernoulli.rvs(y))

dataset["ip"] = np.array(dataset["ip"])
dataset["op"] = np.array(dataset["op"])
model.update_matrix(dataset["ip"])
print("matrix updated")
t_term = log(t)
ro = [sqrt((float(ro_i) / 100) * t_term) for ro_i in range(1, 10)]
print ro[:10]
# random.shuffle(ro)
# print ro[:10]
parameters = {'ro' : ro}
print("parameters set for grid search")
# TO DO FIT THE MODEL USING GRID SEARCH
grid = GridSearchCV(model, param_grid=parameters)
print("grid search object created")
grid.fit(dataset["ip"], dataset["op"])
print("fitting dataset")
print "best ro = ", grid.best_estimator_.ro
chosen = []

for i in range(t, 10001):
	print "Doing %d"%i

	# CALCULATION TO BE DONE
	t_term = log(i + 1)
	# print t_term
	ro = [sqrt((float(ro_i) / 100) * t_term) for ro_i in range(1, 10)]
	print ro[:10]
	# random.shuffle(ro)
	# print ro[:10]
	parameters = {'ro' : ro}

	# Predicting and sampling the next best arm
	next_arm = model.predict_arm(model.acquisition)					# Predit arm
	model.update_matrix(next_arm)					# Update design matrix
	chosen.append(np.where(model.arms == next_arm)[0][0])
	y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary
	# Update the dataset with this sample
	Y1 = list(dataset["op"])
	X1 = list(dataset["ip"])
	X1.append(X[j])
	Y.append(y)
	Y1.append(bernoulli.rvs(y))
	dataset["ip"] = np.array(X1)
	dataset["op"] = np.array(Y1)


	# TO DO FIT THE MODEL USING GRID SEARCH
	grid = GridSearchCV(model, param_grid=parameters)
	grid.fit(dataset["ip"], dataset["op"])
	print "best ro = ", grid.best_estimator_.ro

	x_plot.append(i)
w_sort = [i[0] for i in sorted(enumerate(list(adversary.w_star_)), key=lambda x:x[1])]
print w[:10]
print "best ro = ", grid.best_estimator_.ro
plt.plot(x_plot, chosen, 'ro')
plt.show()









for ro_k in range(1, 11):
	print "iter ", ro_k
	const = float(ro_k) / 10
	# print("Creating object")
	ro = sqrt(const * log(50))
	model = GLM_MAB(arms = X, w_hat = np.random.multivariate_normal(mean, covariance, size = 1)[0], ro = ro)
	# print("object created")
	# print(hasattr(model, 'get_params'))


	dataset = {}
	dataset["ip"] = []
	dataset["op"] = []
	Y = []
	t = 50

	for i in range(t):
		j = random.randint(0, 999)
		dataset["ip"].append(model.arms[j])
		y = adversary.get_adversary_reward(model.arms[j])
		Y.append(y)
		dataset["op"].append(bernoulli.rvs(y))

	dataset["ip"] = np.array(dataset["ip"])
	dataset["op"] = np.array(dataset["op"])
	model.update_matrix(dataset["ip"])
	# print("matrix updated")
	model.fit(dataset["ip"], dataset["op"])
	chosen = []
	cum_regret = np.linalg.norm(adversary.w_star_ - model.w_hat)

	for i in range(t, 1000):
		# if(i % 100 == 0):
		# 	print "Doing %d"%i

		ro = sqrt(const * log(i + 1))
		param = {'ro' : ro}
		model = model.set_params(param)
		# print model.solver

		# Predicting and sampling the next best arm
		next_arm = model.predict_arm(model.acquisition)					# Predit arm
		model.update_matrix(next_arm)					# Update design matrix
		chosen.append(np.where(model.arms == next_arm)[0][0])
		y = adversary.get_adversary_reward(next_arm)	# Sample and get reward from adversary

		Y1 = list(dataset["op"])
		X1 = list(dataset["ip"])
		X1.append(X[j])
		Y.append(y)
		Y1.append(bernoulli.rvs(y))
		dataset["ip"] = np.array(X1)
		dataset["op"] = np.array(Y1)

		model.fit(dataset["ip"], dataset["op"])
		cum_regret += np.linalg.norm(adversary.w_star_ - model.w_hat)
	x_plot.append(const)
	avg_reg = .001 * cum_regret
	print avg_reg
	y_plot.append(avg_reg)





































HELPER BACK UP






'''
   FUNCTION TO GENERATE DATASET FOR THE ADVERSARY

   ARGUMENTS: NONE

   RETURNS: DICT
	Dataset, which is a dictionary with input point as key and output as value

def simulate_data():
	# Fix a w_star
	mean = np.zeros(1000)
	covariance = np.random.rand(1000, 1000)
	w_star = np.random.multivariate_normal(mean, covariance, size = 1)[0]

	dataset = {}
	for i in range(10000):
		while (True):
			x = np.random.multivariate_normal(mean, covariance, size = 1)[0]		# Pick an input
			if (x not in dataset):
				y = link_func("logistic", x, w_star) + np.random.normal(0, 1, 1)[0]             # Generate it's output
				dataset[x] = y
				break

	return dataset, w_star
'''






















import numpy as np, inspect, helper, sys
from math import fabs, log, sqrt
from sklearn import linear_model as LM
from time import time

class GLM_MAB:
	"""Multi Arm Bandit Problem in Generalised Linear Models.
    This class implements Generalised Linear Models using the
    UCB, Thompson Sampling algorithms.
	Parameters
    ----------
	arms	: array, shape (n_arms, n_features) Set of arms for the UCB
			  algorithm.
    algo	: string, Algorithm used to solve the MAB problem. 'UCB' and
			  'Thompson Sampling' are the options
    solver 	: string, default: "logistic regression"
			  Model to estimate the best w_hat to be used for arm prediction
	ro		: float, exploration parameter to give weight to exploration in UCB
	nu		: float, covariance parameter to give weight to covariance in TS
    Attributes
    ----------
    w_hat	: array, shape (1, n_features) The leaning parameter for the
			  algorithm.
    """


	def __init__(self, arms, algo = 'lazy_UCB', ro = 0.5, nu = 0.5,
					solver = "logistic", warm_start = True):
		self.algo = algo
		self.arms = arms
		n_samples, n_features = self.arms.shape
		self.ro = ro
		self.nu = nu
		self.f = np.zeros(n_features,)
		self.w_hat = np.zeros(n_features,)
		self.M_ = np.eye(n_features)
		self.M_inv_ = np.eye(n_features)
		if (solver == "logistic"):
			self.model_ = LM.LogisticRegression(solver = 'newton-cg', C = 0.5)
			# self.model_ = LM.LogisticRegression(solver = 'newton-cg', \
												# warm_start = warm_start)
			self.link_ = "logistic"
		elif (solver == "linear"):
			self.model_ = LM.Ridge()
			self.link_ = "identity"
		else:
			assert False, "THIS MODEL NOT DEFINED"


	def fit(self, X, Y):
		"""Fit the model according to the given training data.
        Parameters
        ----------
        X : array, shape (n_samples, n_features) Training vector, where
			n_samples is the number of samples and n_features is the
			number of features.
        y : array-like, shape (n_samples, 1)
            Target vector relative to X.
        Returns
        -------
        self : object, Returns MAB estimator.
        """
		self.model_.fit(X, Y)
		self.w_hat = self.model_.coef_
		return self


	# def update_matrix(self, X, mu):
	def update_matrix(self, X):
		"""Update the self design matrix after new samples
        Parameters
        ----------
        X : array, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.
        Returns
        -------
        self.M_inv_ : array, shape(n_features, n_features)
            Returns the design matrix.
        """
		# mu = helper.link_func(self.link_, np.dot(np.squeeze(X), np.squeeze(self.w_hat)))
		# self.M_ = self.M_ + mu * (1 - mu) * np.outer(X, X.transpose())
		# print X.shape
		n_samples, n_features = X.shape
		mu = []
		for i in range(n_samples):
			mu.append(helper.link_func(self.link_, np.dot(X[i], np.squeeze(self.w_hat))))
		# print mu
		mu = np.asmatrix(mu)
		mu2 = np.asmatrix(np.ones(n_samples)) - np.asmatrix(mu)
		mu = mu * X
		# print mu.shape
		mu2 = mu2 * X
		# print mu2.shape
		# print mu.T.shape
		sm = np.asarray(mu.T * mu2)
		# print sm.shape, sm[0]
		for i in range(n_features):
			# print self.M_[i][i], sm[i][i]
			self.M_[i][i] = self.M_[i][i] + sm[i][i]
		self.M_inv_ = np.linalg.inv(self.M_)
		# self.M_inv_ = helper.Lin_Alg.update_mat_inv(self.M_inv_, X, mu)


	def update(self, context, reward, choices = []):
		if self.link_ == "identity":
			self.update_matrix(context, reward)
			mu = helper.link_func(self.link_, np.dot(context, self.w_hat))
			self.f = self.f + reward * mu * (1 - mu) * context
			self.w_hat = np.dot(self.M_inv_, self.f.transpose())
		elif self.link_ == "logistic":
			l = len(choices)
			S = np.zeros((l, l))
			n_samples, n_features = self.arms.shape
			X = self.arms[choices[0]].reshape(1, n_features)
			mu = helper.link_func(self.link_, np.dot(self.arms[choices[0]], self.w_hat))
			S[0][0] = mu * (1 - mu)
			for i in range(1, l):
				X = np.concatenate((X, self.arms[choices[i]].reshape(1, n_features)), axis = 0)
				mu = helper.link_func(self.link_, np.dot(self.arms[choices[i]], self.w_hat))
				S[i][i] = mu * (1 - mu)
			X = np.asmatrix(X)
			S = np.asmatrix(S)
			self.M_ = X.T * S * X + np.asmatrix(np.eye(n_features))
			self.M_inv_ = np.linalg.inv(self.M_)
			self.w_hat = np.asarray(self.M_inv_ * X.T * S * np.asmatrix(reward).T)



	def predict_arm(self, acquisition_function):
		"""Function to predict next arm to be sampled
        Parameters
        ----------
		acquisition_function : callable, Function to balance exploitation
							   and exploration.
        Returns
        -------
        self.arm : array, shape(1, n_features)
            Returns the best arm to be pulled.
        """
		if self.algo == 'lazy_UCB':
			rewards = []
			# print "============================================="
			for arm in self.arms:
				rw = acquisition_function(arm)
				# print rw
				rewards.append(acquisition_function(arm))
			ch = np.argmax(np.asarray(rewards))
			# print ch
			# print self.w_hat
			return self.arms[ch]

		if self.algo == 'UCB':
			n_samples, n_features = self.arms.shape
			best_arm = self.arms[0]
			best_mu = helper.link_func(self.link_, np.dot(self.w_hat, self.arms[0]))
			# print self.w_hat.shape, self.w_hat[100]
			for i in range(1, n_samples):
				curr_mu = helper.link_func(self.link_, np.dot(self.w_hat, self.arms[i]))
				if curr_mu > best_mu:
					best_mu = curr_mu
					best_arm = self.arms[i]
			# print best_arm.shape
			self.update_matrix(best_arm.reshape(1, n_features))
			vec = np.dot(self.M_, best_arm.reshape(n_features,))
			# self.w_hat += (self.ro / np.linalg.norm(vec)) * vec
			self.w_hat += vec / np.linalg.norm(vec)
			self.w_hat /= np.linalg.norm(self.w_hat)
			return best_arm

		if self.algo == 'lazy_TS':
			# Get w_tilda from normal centered at w_hat
			try:
				n_samples, n_features = self.arms.shape
				w_tilde = np.random.multivariate_normal(np.squeeze(self.w_hat)\
							, self.nu * self.nu  * self.M_inv_, 1)[0]
				ch = np.argmax(np.dot(self.arms, w_tilde))
				return self.arms[ch], ch
			except:
				print self.M_inv_.shape
				sys.exit(0)

			# # Get arm which maximizes the dot product with the w_hat in case of finite arms
			# if n_features < float("inf"):
			# 	ip = np.dot(self.arms, w_tilde)
			# 	rewards = []
			# 	for i in ip:
			# 		rewards.append(helper.link_func(self.link_, i))
			# 	return self.arms[np.argmax(np.asarray(rewards))]
			# # Get arm as the unit vector along the w_hat in case of infinite arms
			# else:
			# 	return w_tilde / np.linalg.norm(w_tilde)

		assert False, "Algo not defined"



	def acquisition(self, arm):
		"""Acquisition function to balance exploitation and exploration
        Parameters
        ----------
		arm : array, shape(1, n_features) Arm for which to calculate the
			  score of acquisition function
        Returns
        -------
        exploitation + exploration : float,
            						Returns the acquisition score.
        """
		mu = helper.link_func(self.link_, np.dot(self.w_hat.transpose(), arm))
		# mu = np.dot(arm, np.squeeze(self.w_hat))
		explore = self.ro * np.sqrt(np.dot(np.dot(arm.transpose(), self.M_inv_), arm))

		return mu + explore




class Adversary:
	def __init__(self, w_star, X, model = "logistic"):
		self.w_star_ = w_star
		if (model == "logistic"):
			self.link_ = "logistic"
		elif (model == "linear"):
			self.link_ = "identity"
		else:
			assert False, "THIS MODEL NOT DEFINED"
		self.a_star_reward = np.amax(self.get_adversary_reward(X))

	def get_adversary_reward(self, X):
		n_samples, n_features = X.shape
		ip = np.dot(X, self.w_star_)
		rewards = []
		for i in ip:
			rewards.append(helper.link_func(self.link_, i))
		return np.asarray(rewards)





























		from GLM_MAB import GLM_MAB, Adversary
		from learner import MAB_GridSearch as gs
		from math import log, sqrt
		import helper
		import matplotlib.pyplot as plt
		import matplotlib.patches as mpatches
		from scipy.stats import bernoulli
		import numpy as np, pylab, random, pickle
		from time import time

		my_features = 25

		# Generate w_star for adversary
		mean = np.zeros(my_features)
		covariance = np.random.rand(my_features, my_features)
		covariance = np.dot(covariance, covariance.transpose()) + np.eye(my_features)
		w_star = np.random.multivariate_normal(mean, covariance, 1)[0]
		# w_star = np.random.multivariate_normal(np.zeros(my_features), np.eye(my_features), 1)[0]
		w_star = w_star / np.linalg.norm(w_star)

		# Get arms
		X = np.random.multivariate_normal(mean, covariance, 100)
		# X = np.random.multivariate_normal(np.zeros(my_features), np.eye(my_features), 100)
		n_samples, n_features = X.shape
		for i in range(n_samples):
			X[i] = X[i] / np.linalg.norm(X[i])
		# X = pickle.load(open("./X.p", "rb"))
		# w_star = pickle.load(open("./ws.p", "rb"))

		adversary = Adversary(w_star, X, model = "logistic")
		# # a_star_reward = np.amax(np.dot(X, w_star))
		#
		# y_plot = []
		# # colors = ['red', 'green', 'blue', 'black']
		# # labels = []
		# colors = ['red', 'green']
		# labels = ["lazy_TS", "lazy_UCB"]
		# plt.subplot(211)
		# lines = [0, 0]
		# i = -1
		# for algo in labels:
		# 	i += 1
		# 	model = GLM_MAB(X, algo = algo, solver = "logistic")
		# 	yp = []
		# 	regret = 0.0
		# 	avg_regret = 0.0
		# 	for t in range(1000):
		# 		next_arm = model.predict_arm(model.acquisition)
		# 		next_arm = next_arm.reshape(1, n_features)
		# 		reward = adversary.get_adversary_reward(next_arm)
		# 		# reward = np.dot(next_arm, w_star)
		# 		model.update(next_arm, reward[0])
		# 		regret += adversary.a_star_reward - reward[0]
		# 		avg_regret = regret / (t + 1)
		# 		yp.append(regret)
		# 	print avg_regret
		# 	lines[i], = plt.plot(range(1, 1001), yp, label = labels[i], color = colors[i])
		# 	y_plot.append([yp[ypi] / (ypi + 1) for ypi in range(len(yp))])
		# plt.ylabel("Cumulative Regret")
		# plt.xlabel("time steps")
		# plt.subplot(212)
		# i = -1
		# for yp in y_plot:
		# 	i += 1
		# 	lines[i], = plt.plot(range(1, 1001), yp, label = labels[i], color = colors[i])
		# plt.ylabel("Average Regret")
		# plt.xlabel("time steps")
		# plt.legend()
		# plt.show()








		y_plot = []
		# colors = ['red', 'green', 'blue', 'black']
		colors = ['red', 'green', 'blue', 'black', 'brown', 'pink', 'orange', 'violet', 'cyan', 'yellow']
		labels = []
		# colors = ['red', 'green']
		# labels = ["linTS", "logit_TS"]
		# plt.subplot(211)
		# plt.title("smoothness factor = 0.2")
		# lines = [0, 0]
		# model1 = GLM_MAB(X, algo = "lazy_TS", solver = "logistic")
		# model2 = GLM_MAB(X, algo = "lazy_TS", solver = "logistic")
		# yp1 = []
		# regret1 = 0.0
		# avg_regret1 = 0.0
		# yp2 = []
		# regret2 = 0.0
		# avg_regret2 = 0.0
		# dataset = {}
		# dataset['ip'] = []
		# dataset['op'] = []
		# h = np.zeros(100)
		# for t in range(5):
		# 	s = int(np.random.uniform(1,100,1)[0]) - 1
		# 	while h[s] != 0:
		# 		s = np.random.uniform(1,100,1)[0] - 1
		# 	h[s] = 1
		# 	model2.update_matrix(X[s])
		# 	reward = adversary.get_adversary_reward(X[s].reshape(1, n_features))
		# 	dataset['ip'].append(np.squeeze(X[s]))
		# 	dataset['op'].append(bernoulli.rvs(reward, size = 1)[0])
		# 	regret2 += adversary.a_star_reward - reward[0]
		# 	avg_regret2 = regret2 / (t + 1)
		# 	yp2.append(regret2)
		# 	next_arm = model1.predict_arm(model1.acquisition)
		# 	next_arm = next_arm.reshape(1, n_features)
		# 	reward = adversary.get_adversary_reward(next_arm)
		# 	# reward = np.dot(next_arm, w_star)
		# 	model1.update(next_arm, reward[0])
		# 	regret1 += adversary.a_star_reward - reward[0]
		# 	avg_regret1 = regret1 / (t + 1)
		# 	yp1.append(regret1)
		# for t in range(5, 1000):
		# 	next_arm = model2.predict_arm(model2.acquisition)
		# 	next_arm = next_arm.reshape(1, n_features)
		# 	reward = adversary.get_adversary_reward(next_arm)
		# 	model2.update_matrix(next_arm)
		# 	reward = adversary.get_adversary_reward(next_arm)
		# 	dataset['ip'].append(np.squeeze(next_arm))
		# 	dataset['op'].append(bernoulli.rvs(reward, size = 1)[0])
		# 	regret2 += adversary.a_star_reward - reward[0]
		# 	avg_regret2 = regret2 / (t + 1)
		# 	yp2.append(regret2)
		# 	next_arm = model1.predict_arm(model1.acquisition)
		# 	next_arm = next_arm.reshape(1, n_features)
		# 	reward = adversary.get_adversary_reward(next_arm)
		# 	model1.update(next_arm, reward[0])
		# 	regret1 += adversary.a_star_reward - reward[0]
		# 	avg_regret1 = regret1 / (t + 1)
		# 	yp1.append(regret1)
		# print avg_regret1, avg_regret2
		# lines[0], = plt.plot(range(1, 1001), yp1, label = labels[0], color = colors[0])
		# lines[1], = plt.plot(range(1, 1001), yp2, label = labels[1], color = colors[1])
		# y_plot.append([yp1[ypi] / (ypi + 1) for ypi in range(len(yp1))])
		# y_plot.append([yp2[ypi] / (ypi + 1) for ypi in range(len(yp2))])
		# plt.ylabel("Cumulative Regret")
		# plt.xlabel("time steps")
		# plt.subplot(212)
		# i = -1
		# for yp in y_plot:
		# 	i += 1
		# 	lines[i], = plt.plot(range(1, 1001), yp, label = labels[i], color = colors[i])
		# plt.ylabel("Average Regret")
		# plt.xlabel("time steps")
		# plt.legend()
		# plt.show()








		plt.subplot(211)
		lines = [0 for i in range(1, 11)]

		# for nu in [5, 6, 8, 9, 10, 13, 14, 15, 16, 17]:
		# for nu in [.5]:
		i = -1
		# for nu in range(1, 11):
		for nu in [16]:
			i += 1
			nu = float(nu) / 10
			model = GLM_MAB(X, algo = "lazy_TS", nu = nu, solver = "logistic")
			yp = []
			regret = 0.0
			avg_regret = 0.0
			dataset = {}
			dataset['ip'] = []
			dataset['op'] = []
			h = np.zeros(100)
			choices = []
			for t in range(10):
				s = int(np.random.uniform(1,100,1)[0]) - 1
				while h[s] != 0:
					s = int(np.random.uniform(1,100,1)[0]) - 1
				h[s] = 1
				reward = adversary.get_adversary_reward(X[s].reshape(1, n_features))
				choices.append(list(X[s]))
				dataset['ip'].append(np.squeeze(X[s]))
				dataset['op'].append(bernoulli.rvs(reward, size = 1)[0])
				regret += adversary.a_star_reward - reward[0]
				avg_regret = regret / (t + 1)
				yp.append(regret)

			model.fit(dataset['ip'], dataset['op'])
			model.update_matrix(np.asmatrix(choices))

			# reward = None
			# for t in range(10, 1000):
			for t in range(10, 1000):
				# print t
				next_arm, ch = model.predict_arm(model.acquisition)
				next_arm = next_arm.reshape(1, n_features)
				reward = adversary.get_adversary_reward(next_arm)
				choices.append(list(np.squeeze(next_arm)))
				# if t == 0:
				# 	reward = adversary.get_adversary_reward(next_arm)
				# else:
				# 	rt = adversary.get_adversary_reward(next_arm)
				# 	reward = np.concatenate((reward, rt), axis = 0)
				dataset['ip'].append(np.squeeze(next_arm))
				dataset['op'].append(bernoulli.rvs(reward, size = 1)[0])
				model.fit(dataset['ip'], dataset['op'])
				# print np.asmatrix(choices)
				model.update_matrix(np.asmatrix(choices))
				# model.update(next_arm, reward, choices)
				# model.update(next_arm, reward)
				regret += adversary.a_star_reward - reward[0]
				if t % 100 == 0:
					print regret, t / 100
				avg_regret = regret / (t + 1)
				yp.append(regret)
			print avg_regret
			labels.append("nu = " + str(nu))
			lines[i], = plt.plot(range(1, 1001), yp, label = labels[i], color = colors[i])
			y_plot.append([yp[ypi] / (ypi + 1) for ypi in range(len(yp))])
		plt.ylabel("Cumulative Regret")
		plt.xlabel("time steps")

		plt.subplot(212)
		i = -1
		for yp in y_plot:
			i += 1
			lines[i], = plt.plot(range(1, 1001), yp, label = labels[i], color = colors[i])
		plt.ylabel("Average Regret")
		plt.xlabel("time steps")
		plt.legend()
		plt.show()
